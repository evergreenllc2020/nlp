{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis app \n",
    "### Author : Evergreen Technologies \n",
    "\n",
    "#### This script loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset  (classification of newsgroup messages into 20 different categories).\n",
    "\n",
    "#### '''GloVe embedding data can be found at:  http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "#### 20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath):\n",
    "     if os.path.splitext(filepath)[1] != '.csv':\n",
    "          return  # or whatever\n",
    "     seps = [',', ';', '\\t']                    # ',' is default\n",
    "     encodings = [None, 'utf-8', 'ISO-8859-1']  # None is default\n",
    "     for sep in seps:\n",
    "         for encoding in encodings:\n",
    "              try:\n",
    "                  return pd.read_csv(filepath, encoding=encoding, sep=sep)\n",
    "              except Exception:  # should really be more specific \n",
    "                  pass\n",
    "     raise ValueError(\"{!r} is has no encoding in {} or seperator in {}\"\n",
    "                      .format(filepath, encodings, seps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = '/Volumes/My Passport for Mac/data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'imdb_movie_reviews')\n",
    "MOVIE_REVIEW_FILE_NAME = \"imdb_master.csv\"\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "input_df = read_csv(os.path.join(TEXT_DATA_DIR, MOVIE_REVIEW_FILE_NAME))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = input_df[['review','label']]\n",
    "review_df = shuffle(review_df)\n",
    "print(\"Here are Few Samples in data\")\n",
    "print(review_df.head)\n",
    "\n",
    "print(\"Here total number of posistive, negative and unsupported (neutral) samples\")\n",
    "print(review_df.groupby(['label']).count())\n",
    "\n",
    "print(\"Converting pandas dataframe into lists\")\n",
    "texts = review_df['review'].values.tolist()\n",
    "labels = []\n",
    "labels_text = []\n",
    "labels_text_unique = review_df.label.unique().tolist()\n",
    "labels_text = review_df['label'].values.tolist()\n",
    "\n",
    "idxCounter = 0\n",
    "for label in labels_text_unique:\n",
    "    labels_index[label] = idxCounter\n",
    "    idxCounter = idxCounter + 1;\n",
    "\n",
    "idxCounter = 0    \n",
    "for label in labels_text:\n",
    "    print(\"processing row \" + str(idxCounter))\n",
    "    labels.append(labels_index[label])\n",
    "    idxCounter = idxCounter + 1;\n",
    "    \n",
    "\n",
    "print(\"Labels Array\")\n",
    "print(len(labels))\n",
    "print(\"Labels Dictionary\")\n",
    "print(labels_index)\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# second, prepare text samples and their labels\n",
    "#print('Processing text dataset')\n",
    "\n",
    "#texts = []  # list of text samples\n",
    "#labels_index = {}  # dictionary mapping label name to numeric id\n",
    "#labels = []  # list of label ids\n",
    "#for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "#    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "#    if os.path.isdir(path):\n",
    "#        label_id = len(labels_index)\n",
    "#        labels_index[name] = label_id\n",
    "#       for fname in sorted(os.listdir(path)):\n",
    "#            if fname.isdigit():\n",
    "#                fpath = os.path.join(path, fname)\n",
    "#                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "#                with open(fpath, **args) as f:\n",
    "#                    t = f.read()\n",
    "#                    i = t.find('\\n\\n')  # skip header\n",
    "#                    if 0 < i:\n",
    "#                        t = t[i:]\n",
    "#                    texts.append(t)\n",
    "#                labels.append(label_id)\n",
    "\n",
    "#print('Found %s texts.' % len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_val[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(512, 5, activation='relu')(embedded_sequences)\n",
    "x = Dropout(0.3)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(512, 5, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(512, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=500,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "pyplot.plot(history.history[\"f1_m\"],  label=\"f1 score\")\n",
    "pyplot.plot(history.history[\"precision_m\"], label=\"precision\")\n",
    "pyplot.plot(history.history[\"recall_m\"], label=\"recall\")\n",
    "pyplot.plot(history.history['acc'] , label=\"accuracy\")\n",
    "pyplot.title('Evaluation stats')\n",
    "pyplot.legend(loc=\"upper left\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
